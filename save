nh_attn_forward_q_k = '''
extern "C"
__global__ void nh_attn_forward_q_k(const ${Dtype}* query, const ${Dtype}* key, const ${Dtype}* bias, ${Dtype}* attn, \
                                    const int nthreads, const int height, const int width, \
                                    const int strid_channel[4],const int strid_window[4],const int strid_bias[2]) {         
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_window[0];
        const int n_h = (index / strid_window[1]) % ${num_heads};
        const int h = (index / strid_window[2]) % height;
        const int w = (index / strid_window[3]) % width;
        const int k = index % strid_window[3];
        
        const int kh = k / ${window_size};
        const int kw = k % ${window_size};
        
        int ph = ${neighborhood_size};
        int pw = ${neighborhood_size};
        int nh = h - ${neighborhood_size};
        int nw = w - ${neighborhood_size};
        
        if (nh < 0){
            nh = 0;
            ph = ${w_size_m_1} - h;
        }
        else if (h + ${neighborhood_size} >= height){
            nh = height - ${window_size};
            ph = height - 1 - h;
        }
        
        if (nw < 0){
            nw = 0;
            pw = ${w_size_m_1} - w;
        }
        else if (w + ${neighborhood_size} >= width){
            nw = height - ${window_size};
            pw = height - 1 - w;
        }
        
        const int batch_idx = b * strid_channel[0] + n_h * strid_channel[1];
        int q_idx = batch_idx + h * strid_channel[2] + w * strid_channel[3];
        int k_idx = batch_idx + (kh+nh) * strid_channel[2] + (kw+nw) * strid_channel[3];
        const int b_idx = n_h * strid_bias[0] + (ph+kh) * strid_bias[1] + (pw+kw);
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int c=0; c < strid_channel[3]; ++c){
            update_value += query[q_idx] * key[k_idx];
            ++q_idx;
            ++k_idx;
        }
        update_value += bias[b_idx];
        attn[index] = update_value;
    }
}
'''
nh_attn_backward_query = '''
extern "C"
__global__ void nh_attn_backward_query(const ${Dtype}* const key, const ${Dtype}* const d_attn, ${Dtype}* const d_query, \
                                       const int nthreads, const int height, const int width, \
                                       const int strid_channel[4],const int strid_window[4])) {
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_channel[0];
        const int n_h = (index / strid_channel[1]) % ${num_heads};
        const int h = (index / strid_channel[2]) % height;
        const int w = (index / strid_channel[3]) % width;
        const int c = index % strid_channel[3];
        
        int nh = max(h - ${neighborhood_size}, 0) + (h + ${neighborhood_size} >= height) * (height - h - ${nh_size_m_1});
        int nw = max(w - ${neighborhood_size}, 0) + (w + ${neighborhood_size} >= width) * (width - w - ${nh_size_m_1});
        
        int a_idx = b * strid_window[0] + n_h * strid_window[1] + h * strid_window[2] + w * strid_window[3];
        const int k_offset = b * strid_channel[0] + n_h * strid_channel[1] + c;
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int xh=nh; xh < nh + ${window_size}; ++xh){
            #pragma unroll
            for (int xw=nw; xw < nw + ${window_size}; ++xw){
                const int k_idx = k_offset + xh * strid_channel[2] + xw * strid_channel[3];
                update_value += d_attn[a_idx] * key[k_idx];
                ++a_idx;
            }
        }
        d_query[index] = update_value;
    }
}
'''

nh_attn_backward_key = '''
extern "C"
__global__ void nh_attn_backward_key(const ${Dtype}* const query, const ${Dtype}* const d_attn, ${Dtype}* const d_key, \ 
                                     const int nthreads, const int height, const int width, \
                                     const int strid_channel[4],const int strid_window[4]) {
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_channel[0];
        const int n_h = (index / strid_channel[1]) % ${num_heads};
        const int h = (index / strid_channel[2]) % height;
        const int w = (index / strid_channel[3]) % width;
        const int c = index % strid_channel[3];
        
        int nh = max(h - ${neighborhood_size}, 0) + (h + ${neighborhood_size} >= height) * (height - h - ${nh_size_m_1});
        int nw = max(w - ${neighborhood_size}, 0) + (w + ${neighborhood_size} >= width) * (width - w - ${nh_size_m_1});
        
        int a_idx = b * strid_window[0] + n_h * strid_window[1] + h * strid_window[2] + w * strid_window[3];
        const int k_offset = b * strid_channel[0] + n_h * strid_channel[1] + c;
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int xh=nh; xh < nh + ${window_size}; ++xh){
            #pragma unroll
            for (int xw=nw; xw < nw + ${window_size}; ++xw){
                const int k_idx = k_offset + xh * strid_channel[2] + xw * strid_channel[3];
                atomicAdd(&d_key[k_idx], query[index] * d_attn[a_idx]);
                ++a_idx;
            }
        }
    }
}
'''

nh_attn_backward_bias = '''
extern "C"
__global__ void nh_attn_backward_bias(const ${Dtype}* const d_attn, ${Dtype}* const d_bias, \
                                      const int nthreads, const int height, const int width, \
                                      const int strid_window[4], const int strid_bias[2]) {
      CUDA_KERNEL_LOOP(index, nthreads) {
        const int n_h = (index / strid_window[1]) % ${num_heads};
        const int h = (index / strid_window[2]) % height;
        const int w = (index / strid_window[3]) % width;
        const int k = index % strid_window[3];
        
        const int kh = k / ${window_size};
        const int kw = k % ${window_size};
        
        int ph = ${neighborhood_size};
        int pw = ${neighborhood_size};
        
        if (h < ${neighborhood_size}){
            ph = ${w_size_m_1} - h;
        }
        else if (h + ${neighborhood_size} >= height){
            ph = height - 1 - h;
        }
        
        if (w < ${neighborhood_size}){
            pw = ${w_size_m_1} - w;
        }
        else if (w + ${neighborhood_size} >= width){
            pw = width - 1 - w;
        }
        
        const int b_idx = n_h * strid_bias[0] + (ph+kh) * strid_bias[1] + (pw+kw);
        atomicAdd(&d_bias[b_idx], d_attn[index]);
    }
}
'''
nh_attn_forward_attn_v = '''
extern "C"
__global__ void nh_attn_forward_attn_v(const ${Dtype}* attn, const ${Dtype}* value, ${Dtype}* out, \
                                       const int nthreads, const int height, const int width, \
                                       const int strid_channel[4],const int strid_window[4]) {
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_channel[0];
        const int n_h = (index / strid_channel[1]) % ${num_heads};
        const int h = (index / strid_channel[2]) % height;
        const int w = (index / strid_channel[3]) % width;
        const int c = index % strid_channel[3];
        
        int nh = max(h - ${neighborhood_size}, 0) + (h + ${neighborhood_size} >= height) * (height - h - ${nh_size_m_1});
        int nw = max(w - ${neighborhood_size}, 0) + (w + ${neighborhood_size} >= width) * (width - w - ${nh_size_m_1});
        
        int a_idx = b * strid_window[0] + n_h * strid_window[1] + h * strid_window[2] + w * strid_window[3];
        const int v_offset = b * strid_channel[0] + n_h * strid_channel[1] + c;
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int xh=nh; xh < nh + ${window_size}; ++xh){
            #pragma unroll
            for (int xw=nw; xw < nw + ${window_size}; ++xw){
                const int v_idx = v_offset + xh * strid_channel[2] + xw * strid_channel[3];
                update_value += attn[a_idx] * value[v_idx];
                ++a_idx;
            }
        }
        out[index] = update_value;
    }
}
'''
nh_attn_backward_attn = '''
extern "C"
__global__ void nh_attn_backward_attn(const ${Dtype}* const value, const ${Dtype}* const d_out, ${Dtype}* const d_attn, \
                                      const int nthreads, const int height, const int width, \
                                      const int strid_channel[4],const int strid_window[4]) {
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_window[0];
        const int n_h = (index / strid_window[1]) % ${num_heads};
        const int h = (index / strid_window[2]) % height;
        const int w = (index / strid_window[3]) % width;
        const int k = index % strid_window[3];
        
        const int kh = k / ${window_size};
        const int kw = k % ${window_size};
        
        int nh = max(h - ${neighborhood_size}, 0) + (h + ${neighborhood_size} >= height) * (height - h - ${nh_size_m_1});
        int nw = max(w - ${neighborhood_size}, 0) + (w + ${neighborhood_size} >= width) * (width - w - ${nh_size_m_1});
        
        const int batch_idx = b * strid_channel[0] + n_h * strid_channel[1];
        int o_idx = batch_idx + h * strid_channel[2] + w * strid_channel[3];
        int v_idx = batch_idx + (kh+nh) * strid_channel[2] + (kw+nw) * strid_channel[3];
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int c=0; c < strid_channel[3]; ++c){
            update_value += d_out[o_idx] * value[v_idx];
            ++o_idx;
            ++v_idx;
        }
        d_attn[index] = update_value;
    }
}
'''

nh_attn_backward_value = '''
extern "C"
__global__ void nh_attn_backward_value(const ${Dtype}* const attn, const ${Dtype}* const d_out, ${Dtype}* const d_value \
                                       const int nthreads, const int height, const int width, \
                                       const int strid_channel[4], const int strid_window[4]) {
    CUDA_KERNEL_LOOP(index, nthreads) {
        const int b = index / strid_channel[0];
        const int n_h = (index / strid_channel[1]) % ${num_heads};
        const int h = (index / strid_channel[2]) % height;
        const int w = (index / strid_channel[3]) % width;
        const int c = index % strid_channel[3];
        
        int nh = max(h - ${neighborhood_size}, 0) + (h + ${neighborhood_size} >= height) * (height - h - ${nh_size_m_1});
        int nw = max(w - ${neighborhood_size}, 0) + (w + ${neighborhood_size} >= width) * (width - w - ${nh_size_m_1});
        
        int a_idx = b * strid_window[0] + n_h * strid_window[1] + h * strid_window[2] + w * strid_window[3];
        const int v_offset = b * strid_channel[0] + n_h * strid_channel[1] + c;
        
        ${Dtype} update_value = 0;
        #pragma unroll
        for (int xh=nh; xh < nh + ${window_size}; ++xh){
            #pragma unroll
            for (int xw=nw; xw < nw + ${window_size}; ++xw){
                const int v_idx = v_offset + xh * strid_channel[2] + xw * strid_channel[3];
                atomicAdd(&d_value[v_idx], attn[a_idx] * d_out[index]);
                ++a_idx;
            }
        }
    }
}
'''

kernel_loop = '''
#include <cupy/carray.cuh>
#include <cupy/atomics.cuh>
#define CUDA_KERNEL_LOOP(i, n)                        \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
      i < (n);                                       \
      i += blockDim.x * gridDim.x)
'''

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.cuda.amp import custom_fwd, custom_bwd
from timm.models.layers import trunc_normal_, DropPath
from timm.models.registry import register_model

import cupy
from string import Template

CUDA_NUM_THREADS = 1024

def GET_BLOCKS(N):
    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS

@cupy._util.memoize(for_each_device=True)
def load_kernel(kernel_name, code, **kwargs):
    code = kernel_loop + code
    code = Template(code).substitute(**kwargs)
    return cupy.RawKernel(code,kernel_name)

class nh_attn_function_set():
    def __init__(self,num_heads, channels,window_size):
        self.opt = dict(num_heads=num_heads,window_size=window_size,
                        neighborhood_size=window_size//2,nh_size_m_1 = window_size//2 - 1,
                        w_size_m_1=window_size - 1)
        self.kernel = {}
        self.kernel['nh_attn_forward_q_k'] = self.load_kernel_dtypes('nh_attn_forward_q_k', nh_attn_forward_q_k, self.opt)
        self.kernel['nh_attn_backward_query'] = self.load_kernel_dtypes('nh_attn_backward_query', nh_attn_backward_query, self.opt)
        self.kernel['nh_attn_backward_key'] = self.load_kernel_dtypes('nh_attn_backward_key', nh_attn_backward_key, self.opt)
        self.kernel['nh_attn_backward_bias'] = self.load_kernel_dtypes('nh_attn_backward_bias', nh_attn_backward_bias, self.opt)
        self.kernel['nh_attn_forward_attn_v'] = self.load_kernel_dtypes('nh_attn_forward_attn_v', nh_attn_forward_attn_v, self.opt)
        self.kernel['nh_attn_backward_attn'] = self.load_kernel_dtypes('nh_attn_backward_attn', nh_attn_backward_attn, self.opt)
        self.kernel['nh_attn_backward_value'] = self.load_kernel_dtypes('nh_attn_backward_value', nh_attn_backward_value, self.opt)
        
    def __call__(self,mode,dtype,inputs):
        self.kernel[mode][dtype](**inputs)
        
    def load_kernel_dtypes(self,kernel_name, code, opt):
        kernel_dict = {}
        opt['Dtype'] = 'float16'        
        kernel_dict['float16'] = load_kernel(kernel_name, code, **opt)
        opt['Dtype'] = 'float'
        kernel_dict['float32'] = load_kernel(kernel_name, code, **opt)
        opt['Dtype'] = 'double'
        kernel_dict['float64'] = load_kernel(kernel_name, code, **opt)
        return kernel_dict     
    
        
class nh_attn_q_k(torch.autograd.Function):    
    @staticmethod
    def forward(ctx, query, key, bias,function_set):
        assert query.dim() == 5
        assert key.dim() == 5
        assert bias.dim() == 3
        
        query = query.detach().contiguous()
        key = key.detach().contiguous()
        bias = bias.detach().contiguous()
        batch_size, num_head, height, width , _ = query.shape
        attn = torch.empty((batch_size, num_head,height, width,function_set.opt['window_size']**2),dtype=query.dtype,device=query.device)
        
        ctx.input = query, key, bias
        ctx.height = height
        ctx.width = width

        ctx.stride_channel = cupy.asarray(query.stride()[:-1])
        ctx.stride_window = cupy.asarray(attn.stride()[:-1])
        ctx.stride_bias = cupy.asarray(bias.stride()[:-1])
        ctx.function_set = function_set
        
        query = cupy.from_dlpack(query)
        key = cupy.from_dlpack(key)
        bias = cupy.from_dlpack(bias)
        attn = cupy.from_dlpack(attn)

        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(attn.size),1,1),
                      args=[query, key, bias, attn, attn.size, 
                            ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window,ctx.stride_bias])
        function_set('nh_attn_forward_q_k', query.dtype.name, inputs)

        attn = torch.from_dlpack(attn.toDlpack())
        return attn

    @staticmethod
    def backward(ctx, d_attn):
        query, key, bias = ctx.input
        function_set = ctx.function_set
        
        d_attn = cupy.from_dlpack(d_attn.detach()).ravel()
        query = cupy.from_dlpack(query).ravel()
        key = cupy.from_dlpack(key).ravel()
        bias = cupy.from_dlpack(bias).ravel()

        d_query, d_key, d_bias = None, None, None

        d_query = cupy.zeros_like(query)
        d_key = cupy.zeros_like(key)
        d_bias = cupy.zeros_like(bias)
        
        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(d_query.size),1,1),
                      args=[key, d_attn, d_query,
                            d_query.size,ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window])
        function_set('nh_attn_backward_query',d_query.dtype.name, inputs)

        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(d_key.size),1,1),
                      args=[query, d_attn, d_key,
                            d_key.size,ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window])
        function_set('nh_attn_backward_key',d_key.dtype.name, inputs)

        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(d_attn.size),1,1),
                      args=[d_attn, d_bias,
                            d_attn.size,ctx.height, ctx.width,
                            ctx.stride_window,ctx.stride_bias])
        function_set('nh_attn_backward_bias',d_bias.dtype.name, inputs)
        
        d_query = torch.from_dlpack(d_query.toDlpack())
        d_key = torch.from_dlpack(d_key.toDlpack())
        d_bias = torch.from_dlpack(d_bias.toDlpack())
        return d_query, d_key, d_bias, None

class nh_attn_attn_v(torch.autograd.Function):
    @staticmethod
    def forward(ctx,attn,value,function_set):
        assert attn.dim() == 5
        assert value.dim() == 5
        
        attn = attn.detach().contiguous()
        value = value.detach().contiguous()
        _, _, height, width, _ = value.shape
        
        ctx.function_set = function_set
        ctx.input = attn, value
        ctx.height = height
        ctx.width = width
        ctx.stride_channel = cupy.asarray(value.stride()[:-1])
        ctx.stride_window = cupy.asarray(attn.stride()[:-1])
        
        attn = cupy.from_dlpack(attn)
        value = cupy.from_dlpack(value)
        
        out = cupy.empty_like(value)

        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(out.size),1,1),
                      args=[attn, value, out, 
                            out.size,ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window])
        function_set('nh_attn_forward_attn_v',out.dtype.name, inputs)
        out = torch.from_dlpack(out.toDlpack())
        return out

    @staticmethod
    def backward(ctx, d_out):
        attn, value = ctx.input
        function_set = ctx.function_set
        
        d_out = cupy.from_dlpack(d_out.detach()).ravel()
        attn = cupy.from_dlpack(attn).ravel()
        value = cupy.from_dlpack(value).ravel()
        
        d_attn, d_value = None, None

        d_attn = cupy.zeros_like(attn)
        d_value = cupy.zeros_like(value)
        
        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(d_attn.size),1,1),
                      args=[value, d_out, d_attn, 
                            d_attn.size,ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window])
        function_set('nh_attn_backward_attn',d_attn.dtype.name, inputs)

        inputs = dict(block=(CUDA_NUM_THREADS,1,1),grid=(GET_BLOCKS(d_value.size),1,1),
                      args=[attn, d_out, d_value, 
                            d_value.size,ctx.height, ctx.width,
                            ctx.stride_channel,ctx.stride_window])
        function_set('nh_attn_backward_value',d_value.dtype.name, inputs)
        
        d_attn = torch.from_dlpack(d_attn.toDlpack())
        d_value = torch.from_dlpack(d_value.toDlpack())
        return d_attn, d_value, None
    
