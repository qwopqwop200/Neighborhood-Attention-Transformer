{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9afa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can easily compare the speed and memory usage of swin and Nat.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, trunc_normal_,to_2tuple\n",
    "\n",
    "import cupy\n",
    "from collections import namedtuple\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd989521",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_loop = '''\n",
    "#define CUDA_KERNEL_LOOP(i, n)                        \\\n",
    "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
    "      i < (n);                                       \\\n",
    "      i += blockDim.x * gridDim.x)\n",
    "'''\n",
    "\n",
    "nh_attn_forward_q_k = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_forward_q_k(const ${Dtype}* query, const ${Dtype}* key, const ${Dtype}* bias, ${Dtype}* attn) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${window_seq_length};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${window_seq_length}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${window_seq_length}) % ${height};\n",
    "        const int w = (index / ${window_seq_length}) % ${width};\n",
    "        const int k = index % ${window_seq_length};\n",
    "        \n",
    "        const int kh = (k / ${window_size}) % ${window_size};\n",
    "        const int kw = (k % ${window_size});\n",
    "        \n",
    "        int ph = ${shift_size};\n",
    "        int pw = ${shift_size};\n",
    "        int nh = h - ${shift_size};\n",
    "        int nw = w - ${shift_size};\n",
    "        \n",
    "        if (nh < 0){\n",
    "            nh = 0;\n",
    "            ph = ${center_pos} - h;\n",
    "        }\n",
    "        else if (h + ${shift_size} >= ${height}){\n",
    "            nh = ${height} - ${window_size};\n",
    "            ph = ${height} - h - 1;\n",
    "        }\n",
    "        \n",
    "        if (nw < 0){\n",
    "            nw = 0;\n",
    "            pw = ${center_pos} - w;\n",
    "        }\n",
    "        else if (w + ${shift_size} >= ${width}){\n",
    "            nw = ${width} - ${window_size};\n",
    "            pw = ${width} - w - 1;\n",
    "        }\n",
    "        \n",
    "        const int q_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w) * ${channels};\n",
    "        const int k_idx = (((b * ${num_heads} + n_h) * ${height} + (kh+nh)) * ${width} + (kw+nw)) * ${channels};\n",
    "        const int b_idx = (n_h * ${bias_size} + (ph+kh)) * ${bias_size} + (pw+kw);\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}  && kh < ${window_size} && kw < ${window_size}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int d=0; d < ${channels}; ++d){\n",
    "                update_value += query[q_idx+d] * key[k_idx+d];\n",
    "            }\n",
    "            update_value += bias[b_idx];\n",
    "            attn[index] = update_value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "nh_attn_backward_query = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_backward_query(const ${Dtype}* const key, const ${Dtype}* const d_attn, ${Dtype}* const d_query) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${channels};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${channels}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${channels}) % ${height};\n",
    "        const int w = (index / ${channels}) % ${width};\n",
    "        const int c = index % ${channels};\n",
    "        \n",
    "        int nh = max(h - ${shift_size}, 0) + (h + ${shift_size} >= ${height}) * (${height} - h - ${shift_size} - 1);\n",
    "        int nw = max(w - ${shift_size}, 0) + (w + ${shift_size} >= ${width}) * (${width} - w - ${shift_size} - 1);\n",
    "        \n",
    "        const int a_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w)*${window_seq_length};\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int kh=0, xh=nh; kh < ${window_size}; ++kh, ++xh){\n",
    "                #pragma unroll\n",
    "                for (int kw=0, xw=nw; kw < ${window_size}; ++kw, ++xw){\n",
    "                     const int k_idx = ((((b * ${num_heads} + n_h) * ${height} + xh) * ${width} + xw) * ${channels} + c);\n",
    "                     update_value += d_attn[a_idx+(kh*${window_size}+kw)] * key[k_idx];\n",
    "                }\n",
    "            }\n",
    "            d_query[index] = update_value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "nh_attn_backward_key = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_backward_key(const ${Dtype}* const query, const ${Dtype}* const d_attn, ${Dtype}* const d_key) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${channels};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${channels}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${channels}) % ${height};\n",
    "        const int w = (index / ${channels}) % ${width};\n",
    "        const int c = index % ${channels};\n",
    "        \n",
    "        int nh = max(h - ${shift_size}, 0) + (h + ${shift_size} >= ${height}) * (${height} - h - ${shift_size} - 1);\n",
    "        int nw = max(w - ${shift_size}, 0) + (w + ${shift_size} >= ${width}) * (${width} - w - ${shift_size} - 1);\n",
    "        \n",
    "        const int a_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w) * ${window_seq_length};\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int kh=0, xh=nh; kh < ${window_size}; ++kh, ++xh){\n",
    "                #pragma unroll\n",
    "                for (int kw=0, xw=nw; kw < ${window_size}; ++kw, ++xw){\n",
    "                    const int k_idx = ((((b * ${num_heads} + n_h) * ${height} + xh) * ${width} + xw) * ${channels} + c);\n",
    "                    d_key[k_idx] += query[index] * d_attn[a_idx+(kh*${window_size}+kw)];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "nh_attn_backward_bias = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_backward_bias(const ${Dtype}* const d_attn, ${Dtype}* const d_bias) {\n",
    "      CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int n_h = (index / ${height} / ${width}/ ${window_seq_length}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${window_seq_length}) % ${height};\n",
    "        const int w = (index / ${window_seq_length}) % ${width};\n",
    "        const int k = index % ${window_seq_length};\n",
    "        \n",
    "        const int kh = (k / ${window_size}) % ${window_size};\n",
    "        const int kw = (k % ${window_size});\n",
    "        \n",
    "        int ph = ${shift_size};\n",
    "        int pw = ${shift_size};\n",
    "        \n",
    "        if (h < ${shift_size}){\n",
    "            ph = ${center_pos} - h;\n",
    "        }\n",
    "        else if (h + ${shift_size} >= ${height}){\n",
    "            ph = ${height} - h - 1;\n",
    "        }\n",
    "        \n",
    "        if (w < ${shift_size}){\n",
    "            pw = ${center_pos} - w;\n",
    "        }\n",
    "        else if (w + ${shift_size} >= ${width}){\n",
    "            pw = ${width} - w - 1;\n",
    "        }\n",
    "        \n",
    "        const int b_idx = (n_h * ${bias_size} + (ph+kh)) * ${bias_size} + (pw+kw);\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}  && kh < ${window_size} && kw < ${window_size}){\n",
    "            d_bias[b_idx] += d_attn[index];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "'''\n",
    "nh_attn_forward_attn_v = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_forward_attn_v(const ${Dtype}* attn, const ${Dtype}* value, ${Dtype}* out) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${channels};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${channels}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${channels}) % ${height};\n",
    "        const int w = (index / ${channels}) % ${width};\n",
    "        const int c = index % ${channels};\n",
    "        \n",
    "        int nh = max(h - ${shift_size}, 0) + (h + ${shift_size} >= ${height}) * (${height} - h - ${shift_size} - 1);\n",
    "        int nw = max(w - ${shift_size}, 0) + (w + ${shift_size} >= ${width}) * (${width} - w - ${shift_size} - 1);\n",
    "        \n",
    "        const int a_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w)*${window_seq_length};\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int kh=0, xh=nh; kh < ${window_size}; ++kh, ++xh){\n",
    "                #pragma unroll\n",
    "                for (int kw=0, xw=nw; kw < ${window_size}; ++kw, ++xw){\n",
    "                     const int v_idx = ((((b * ${num_heads} + n_h) * ${height} + xh) * ${width} + xw) * ${channels} + c);\n",
    "                     update_value += attn[a_idx+(kh*${window_size}+kw)] * value[v_idx];\n",
    "                }\n",
    "            }\n",
    "            out[index] = update_value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "'''\n",
    "nh_attn_backward_attn = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_backward_attn(const ${Dtype}* const value, const ${Dtype}* const d_out, ${Dtype}* const d_attn) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${window_seq_length};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${window_seq_length}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${window_seq_length}) % ${height};\n",
    "        const int w = (index / ${window_seq_length}) % ${width};\n",
    "        const int k = index % ${window_seq_length};\n",
    "        \n",
    "        const int kh = (k / ${window_size}) % ${window_size};\n",
    "        const int kw = (k % ${window_size});\n",
    "        \n",
    "        int nh = max(h - ${shift_size}, 0) + (h + ${shift_size} >= ${height}) * (${height} - h - ${shift_size} - 1);\n",
    "        int nw = max(w - ${shift_size}, 0) + (w + ${shift_size} >= ${width}) * (${width} - w - ${shift_size} - 1);\n",
    "        \n",
    "        const int o_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w) * ${channels};\n",
    "        const int v_idx = (((b * ${num_heads} + n_h) * ${height} + (nh+kh))* ${width} + (nw+kw)) * ${channels};\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}  && kh < ${window_size} && kw < ${window_size}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int d=0; d < ${channels}; ++d){\n",
    "                update_value += d_out[o_idx+d] * value[v_idx+d];\n",
    "            }\n",
    "            d_attn[index] = update_value;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "nh_attn_backward_value = kernel_loop + '''\n",
    "extern \"C\"\n",
    "__global__ void nh_attn_backward_value(const ${Dtype}* const attn, const ${Dtype}* const d_out, ${Dtype}* const d_value) {\n",
    "    CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
    "        const int b = index / ${num_heads} / ${height} / ${width} / ${channels};\n",
    "        const int n_h = (index / ${height} / ${width}/ ${channels}) % ${num_heads};\n",
    "        const int h = (index / ${width}/ ${channels}) % ${height};\n",
    "        const int w = (index / ${channels}) % ${width};\n",
    "        const int c = index % ${channels};\n",
    "        \n",
    "        int nh = max(h - ${shift_size}, 0) + (h + ${shift_size} >= ${height}) * (${height} - h - ${shift_size} - 1);\n",
    "        int nw = max(w - ${shift_size}, 0) + (w + ${shift_size} >= ${width}) * (${width} - w - ${shift_size} - 1);\n",
    "        \n",
    "        const int a_idx = (((b * ${num_heads} + n_h) * ${height} + h) * ${width} + w) * ${window_seq_length};\n",
    "        \n",
    "        if (h < ${height} && w < ${width} && n_h < ${num_heads}){\n",
    "            ${Dtype} update_value = 0;\n",
    "            #pragma unroll\n",
    "            for (int kh=0, xh=nh; kh < ${window_size}; ++kh, ++xh){\n",
    "                #pragma unroll\n",
    "                for (int kw=0, xw=nw; kw < ${window_size}; ++kw, ++xw){\n",
    "                     const int v_idx = ((((b * ${num_heads} + n_h) * ${height} + xh) * ${width} + xw) * ${channels} + c);\n",
    "                     d_value[v_idx] += attn[a_idx+(kh*${window_size}+kw)] * d_out[index];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c53b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_NUM_THREADS = 1024\n",
    "Stream = namedtuple('Stream', ['ptr'])\n",
    "\n",
    "def GET_BLOCKS(N):\n",
    "    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n",
    "\n",
    "def Dtype(t):\n",
    "    if isinstance(t, torch.cuda.FloatTensor):\n",
    "        return 'float'\n",
    "    elif isinstance(t, torch.cuda.DoubleTensor):\n",
    "        return 'double'\n",
    "    \n",
    "@cupy._util.memoize(for_each_device=True)\n",
    "def load_kernel(kernel_name, code, **kwargs):\n",
    "    code = Template(code).substitute(**kwargs)\n",
    "    kernel_code = cupy.cuda.compile_with_cache(code)\n",
    "    return kernel_code.get_function(kernel_name)\n",
    "\n",
    "class nh_attn_q_k(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, query, key, bias, window_size):\n",
    "        assert query.dim() == 5 and query.is_cuda\n",
    "        assert key.dim() == 5 and key.is_cuda\n",
    "        assert bias.dim() == 3 and bias.is_cuda\n",
    "        \n",
    "        batch_size, num_heads, height, width ,channels = query.size()\n",
    "        attn = query.new(batch_size, num_heads, height, width,window_size**2)\n",
    "        \n",
    "        with torch.cuda.device_of(query):\n",
    "            n = attn.numel()\n",
    "            opt = dict(Dtype=Dtype(query), nthreads=n,\n",
    "                       batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                       window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                       center_pos=window_size-1, shift_size=window_size//2)\n",
    "            f = load_kernel('nh_attn_forward_q_k', nh_attn_forward_q_k, **opt)\n",
    "            \n",
    "            f(block=(CUDA_NUM_THREADS,1,1),\n",
    "              grid=(GET_BLOCKS(n),1,1),\n",
    "              args=[query.data_ptr(), key.data_ptr(), bias.data_ptr(), attn.data_ptr()],\n",
    "              stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "            \n",
    "        ctx.save_for_backward(query, key, bias)\n",
    "        ctx.window_size = window_size\n",
    "        return attn\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, d_attn):\n",
    "        assert d_attn.is_cuda\n",
    "        \n",
    "        query, key, bias = ctx.saved_tensors\n",
    "        window_size = ctx.window_size\n",
    "        \n",
    "        batch_size, num_heads, height, width ,channels = query.size()\n",
    "        d_query, d_key, d_bias = None, None, None\n",
    "        \n",
    "        with torch.cuda.device_of(d_attn):\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                d_query = query.new(query.size())\n",
    "                n = d_query.numel()\n",
    "                opt = dict(Dtype=Dtype(d_attn), nthreads=n,\n",
    "                           batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                           window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                           center_pos=window_size-1, shift_size=window_size//2)\n",
    "                \n",
    "                f = load_kernel('nh_attn_backward_query',nh_attn_backward_query, **opt)\n",
    "                f(block=(CUDA_NUM_THREADS,1,1),\n",
    "                  grid=(GET_BLOCKS(n),1,1),\n",
    "                  args=[key.data_ptr(), d_attn.data_ptr(), d_query.data_ptr()],\n",
    "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "                \n",
    "            if ctx.needs_input_grad[1]:\n",
    "                d_key = key.new(key.size())\n",
    "                n = d_key.numel()\n",
    "                opt = dict(Dtype=Dtype(d_attn), nthreads=n,\n",
    "                           batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                           window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                           center_pos=window_size-1, shift_size=window_size//2)\n",
    "                \n",
    "                f = load_kernel('nh_attn_backward_key',nh_attn_backward_key, **opt)\n",
    "                f(block=(CUDA_NUM_THREADS,1,1),\n",
    "                  grid=(GET_BLOCKS(n),1,1),\n",
    "                  args=[query.data_ptr(), d_attn.data_ptr(), d_key.data_ptr()],\n",
    "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "                \n",
    "            if ctx.needs_input_grad[2]:\n",
    "                d_bias = bias.new(bias.size())\n",
    "                n = d_attn.numel()\n",
    "                opt = dict(Dtype=Dtype(d_attn), nthreads=n,\n",
    "                           batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                           window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                           center_pos=window_size-1, shift_size=window_size//2)\n",
    "                \n",
    "                f = load_kernel('nh_attn_backward_bias',nh_attn_backward_bias, **opt)\n",
    "                f(block=(CUDA_NUM_THREADS,1,1),\n",
    "                  grid=(GET_BLOCKS(n),1,1),\n",
    "                  args=[d_attn.data_ptr(), d_bias.data_ptr()],\n",
    "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "                \n",
    "        return d_query, d_key, d_bias, None\n",
    "    \n",
    "class nh_attn_attn_v(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,attn,value,window_size):\n",
    "        assert attn.dim() == 5 and attn.is_cuda\n",
    "        assert value.dim() == 5 and value.is_cuda\n",
    "        \n",
    "        batch_size, num_heads, height, width ,channels = value.size()\n",
    "        out = value.new(batch_size, num_heads, height, width, channels)\n",
    "        \n",
    "        with torch.cuda.device_of(attn):\n",
    "            n = out.numel()\n",
    "            opt = dict(Dtype=Dtype(attn), nthreads=n,\n",
    "                       batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                       window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                       center_pos=window_size-1, shift_size=window_size//2)\n",
    "            \n",
    "            f = load_kernel('nh_attn_forward_attn_v', nh_attn_forward_attn_v, **opt)\n",
    "            f(block=(CUDA_NUM_THREADS,1,1),\n",
    "              grid=(GET_BLOCKS(n),1,1),\n",
    "              args=[attn.data_ptr(), value.data_ptr(), out.data_ptr()],\n",
    "              stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "            \n",
    "        ctx.save_for_backward(attn, value)\n",
    "        ctx.window_size = window_size\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, d_out):\n",
    "        assert d_out.is_cuda\n",
    "        \n",
    "        attn, value = ctx.saved_tensors\n",
    "        window_size = ctx.window_size\n",
    "        \n",
    "        batch_size, num_heads, height, width ,channels = value.size()\n",
    "        d_attn, d_value = None, None\n",
    "        \n",
    "        with torch.cuda.device_of(d_out):\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                d_attn = attn.new(attn.size())\n",
    "                n = d_attn.numel()\n",
    "                opt = dict(Dtype=Dtype(d_out), nthreads=n,\n",
    "                           batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                           window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                           center_pos=window_size-1, shift_size=window_size//2)\n",
    "                \n",
    "                f = load_kernel('nh_attn_backward_attn',nh_attn_backward_attn, **opt)\n",
    "                f(block=(CUDA_NUM_THREADS,1,1),\n",
    "                  grid=(GET_BLOCKS(n),1,1),\n",
    "                  args=[value.data_ptr(), d_out.data_ptr(), d_attn.data_ptr()],\n",
    "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "                \n",
    "            if ctx.needs_input_grad[1]:\n",
    "                d_value = value.new(value.size())\n",
    "                n = d_value.numel()\n",
    "                opt = dict(Dtype=Dtype(d_out), nthreads=n,\n",
    "                           batch=batch_size, num_heads=num_heads, height=height, width=width, channels=channels,\n",
    "                           window_size=window_size, window_seq_length=window_size**2, bias_size=(2*window_size-1),\n",
    "                           center_pos=window_size-1, shift_size=window_size//2)\n",
    "                \n",
    "                f = load_kernel('nh_attn_backward_value',nh_attn_backward_value, **opt)\n",
    "                f(block=(CUDA_NUM_THREADS,1,1),\n",
    "                  grid=(GET_BLOCKS(n),1,1),\n",
    "                  args=[attn.data_ptr(), d_out.data_ptr(), d_value.data_ptr()],\n",
    "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
    "                \n",
    "        return d_attn, d_value,None\n",
    "    \n",
    "class NeighborhoodAttention(nn.Module):\n",
    "    def __init__(self,dim, num_heads,window_size=7, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert window_size%2 == 1,'windowsize must be odd.'\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Conv2d(dim,dim*3,1, bias=qkv_bias)\n",
    "        self.proj = nn.Conv2d(dim, dim, 1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.relative_bias = nn.Parameter(torch.zeros(num_heads,(2*self.window_size-1),(2*self.window_size-1)))\n",
    "        \n",
    "        trunc_normal_(self.relative_bias, std=.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.nh_attention(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    def nh_attention(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "        assert H >= self.window_size and W >= self.window_size,'input size must not be smaller than window size'\n",
    "        qkv = self.qkv(x).view(B, 3,self.num_heads,self.head_dim,H,W).permute(1,0,2,4,5,3) # B,nh,H,W,nc\n",
    "        q, k, v = qkv[0], qkv[1] ,qkv[2]\n",
    "        attn = self.nh_attn(q,k,mode='q_k')\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = self.nh_attn(attn,v,mode='attn_v')\n",
    "        out = out.permute(0,1,4,2,3).contiguous().view(B,C,H,W)\n",
    "        return out\n",
    "    \n",
    "    def nh_attn(self,input_1, input_2,mode='q_k'):\n",
    "        if input_1.is_cuda and input_2.is_cuda and self.relative_bias.is_cuda:\n",
    "            if mode.lower() == 'q_k':\n",
    "                attn = nh_attn_q_k.apply(input_1, input_2,self.relative_bias,self.window_size)\n",
    "                return attn\n",
    "            elif mode.lower() == 'attn_v':\n",
    "                out = nh_attn_attn_v.apply(input_1, input_2,self.window_size)\n",
    "                return out\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "class Channel_Layernorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.ln(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "    \n",
    "class Mlp_conv(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features,1)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features,1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class NATLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads,window_size=7,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=Channel_Layernorm, layer_scale=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.attn = NeighborhoodAttention(dim, num_heads,window_size,qkv_bias, qk_scale, attn_drop, drop)\n",
    "        self.mlp = Mlp_conv(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a366339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011700b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_memory_test(model,img):\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    model(img)\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    \n",
    "#https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f\n",
    "def cuda_speed_test(model,img,repetitions):\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True),torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = model(img)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput = (repetitions*batch_size)/total_time\n",
    "    print(f'Final Throughput:{Throughput}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f254c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = 112\n",
    "channel = 64\n",
    "num_head = 4\n",
    "window_size = 7\n",
    "repetitions = 1000\n",
    "test_model = 'nat' #'nat' or 'swin'\n",
    "test_type = 'memory' # 'memory' or 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23eded82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qwopq\\.conda\\envs\\tf\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Throughput:1376.9413621660358\n"
     ]
    }
   ],
   "source": [
    "if test_model.lower() == 'nat':\n",
    "    model = NATLayer(channel,num_head,window_size=window_size).cuda()\n",
    "    img = torch.rand(batch_size,channel,img_size,img_size).cuda()\n",
    "elif test_model.lower() == 'swin':\n",
    "    model = SwinTransformerBlock(channel,(img_size,img_size),num_head,window_size=window_size).cuda()\n",
    "    img = torch.rand(batch_size,img_size*img_size,channel).cuda()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "if test_type.lower() == 'memory':\n",
    "    cuda_memory_test(model,img)\n",
    "elif test_type.lower() == 'speed':\n",
    "    cuda_speed_test(model,img,repetitions)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
